{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This NB just works of an already extracted zip. the directory should be the unzipped directory of that zip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 384093.77it/s]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.16s/it]\n",
      "100%|██████████| 4/4 [00:13<00:00,  3.43s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.56s/it]\n",
      "100%|██████████| 4/4 [00:14<00:00,  3.61s/it]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.33s/it]\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.61s/it]\n",
      "100%|██████████| 4/4 [00:16<00:00,  4.16s/it]\n",
      "100%|██████████| 3/3 [00:12<00:00,  4.04s/it]\n",
      "100%|██████████| 4/4 [00:22<00:00,  5.69s/it]\n",
      "100%|██████████| 4/4 [00:32<00:00,  8.03s/it]\n",
      "100%|██████████| 2/2 [00:22<00:00, 11.45s/it]\n",
      "100%|██████████| 3/3 [00:35<00:00, 11.67s/it]\n"
     ]
    }
   ],
   "source": [
    "directory = '2023-citibike-tripdata/'\n",
    "data = pd.DataFrame()\n",
    "for foldername, subfolders, filenames in os.walk(directory):\n",
    "    for file in tqdm(os.listdir(foldername)):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith('.csv'):\n",
    "            month_df = pd.read_csv(os.path.join(foldername, filename), low_memory=False)\n",
    "            data = pd.concat([data, month_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35107030, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are about 300 rides with dock name starting with 'id'\n",
    "# remove these so columns can be floats\n",
    "\n",
    "data['start_station_id'] = pd.to_numeric(data['start_station_id'], errors='coerce')\n",
    "data['end_station_id'] = pd.to_numeric(data['end_station_id'], errors='coerce')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet('2023-citibike-tripdata/2023_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to filter out self-loops which are quite common\n",
    "filtered_data = data[data['start_station_id'] != data['end_station_id']]\n",
    "unique_trips = filtered_data.groupby(['start_station_id', 'end_station_id']).size().reset_index(name='trip_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join back the lats and longs for each station\n",
    "# the lats/long for station ids differ slightly for each trip, so drop dupes and keep the first since they are all very close\n",
    "\n",
    "starts = data[['start_station_id', 'start_lat', 'start_lng']].drop_duplicates(subset='start_station_id', keep='first')\n",
    "ends = data[['end_station_id', 'end_lat', 'end_lng']].drop_duplicates(subset='end_station_id', keep='first')\n",
    "unique_trips = unique_trips.merge(starts, on='start_station_id', how='left')\n",
    "unique_trips = unique_trips.merge(ends, on='end_station_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_trips.to_parquet('2023-citibike-tripdata/unique_trips.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
